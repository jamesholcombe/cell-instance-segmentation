{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLAB = False\n",
    "# Set this depending on whether running in colab or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\", force_remount=True)\n",
    "\n",
    "    DATAPATH = Path(\"/content/drive/MyDrive/DataSets/data\")\n",
    "else:\n",
    "    DATAPATH = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 13:05:00.767173: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jamesholcombe/git/personal/cell-instance-segmentation/venv/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2021-12-27 13:05:00.767236: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Conv2DTranspose,\n",
    "    GaussianNoise,\n",
    "    Lambda,\n",
    "    Dropout,\n",
    "    UpSampling2D,\n",
    ")\n",
    "from keras import backend as K\n",
    "from keras.optimizer_v2 import adam\n",
    "from keras.metrics import RootMeanSquaredError\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "import math\n",
    "from scipy.sparse import coo_matrix\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation \n",
    "\n",
    "I am defining a few helper functions used as part of the data prep pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_decode(\n",
    "    mask_rle, shape=(520, 704), color=1\n",
    "):  # function to convert tabular mask data to image\n",
    "    \"\"\"\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height, width, channels) of array to return\n",
    "    color: color for the mask\n",
    "    Returns numpy array (mask)\n",
    "\n",
    "    \"\"\"\n",
    "    s = mask_rle.split()\n",
    "\n",
    "    starts = list(map(lambda x: int(x) - 1, s[0::2]))\n",
    "    lengths = list(map(int, s[1::2]))\n",
    "    ends = [x + y for x, y in zip(starts, lengths)]\n",
    "\n",
    "    img = np.zeros((shape[0] * shape[1]), dtype=np.float32)\n",
    "\n",
    "    for start, end in zip(starts, ends):\n",
    "        img[start:end] = color\n",
    "\n",
    "    return img.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroid(arr):\n",
    "    non_zero = np.where(arr == 1)\n",
    "    x, y = non_zero\n",
    "    return int(np.median(x)), int(np.median(y))\n",
    "\n",
    "\n",
    "def make_mask(centroid: tuple):\n",
    "    x, y = centroid\n",
    "    a = np.zeros(\n",
    "        (\n",
    "            int(520 / 8),\n",
    "            int(704 / 8),\n",
    "        )\n",
    "    )\n",
    "    a[int(x / 8), int(y / 8)] = 1\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Data Generator\n",
    "\n",
    "As the dataset is large, I am implementing a custom data generator to load and prepare the data on the fly during model training. This ensures that RAM usage does not exceed hardware limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    \"\"\"Generates data for Keras\n",
    "    Sequence based data generator. Suitable for building data generator for training and prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_directory,\n",
    "        data_path,\n",
    "        to_fit=True,\n",
    "        batch_size=32,\n",
    "        dim=(520, 704),\n",
    "    ):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.sample_ids = self.data[\"id\"].unique()\n",
    "        self.image_indexes = dict(zip(range(len(self.sample_ids)), self.sample_ids))\n",
    "        self.indexes = list(range(len(self.image_indexes)))\n",
    "        self.to_fit = to_fit\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\n",
    "        :return: number of batches per epoch\n",
    "        \"\"\"\n",
    "        return int(np.floor(len(self.sample_ids) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\n",
    "        :param index: index of the batch\n",
    "        :return: X and y when fitting. X only when predicting\n",
    "        \"\"\"\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        X = self._generate_X(indexes)\n",
    "\n",
    "        if self.to_fit:\n",
    "            y = self._generate_y(indexes)\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def _generate_X(self, indexes):\n",
    "        \"\"\"Generates data containing batch_size images\n",
    "        :param list_IDs_temp: list of label ids to load\n",
    "        :return: batch of images\n",
    "        \"\"\"\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, 3))\n",
    "\n",
    "        for i, label in enumerate(indexes):\n",
    "            image_id = self.image_indexes[label]\n",
    "            X[i] = np.repeat(\n",
    "                np.asarray(io.imread(DATAPATH / Path(f\"train/{image_id}.png\")))[\n",
    "                    :, :, np.newaxis\n",
    "                ],\n",
    "                repeats=3,\n",
    "                axis=2,\n",
    "            )\n",
    "        return X\n",
    "\n",
    "    def _generate_y(self, indexes):\n",
    "        \"\"\"Generates data containing batch_size masks\n",
    "        :param list_IDs_temp: list of label ids to load\n",
    "        :return: batch if masks\n",
    "        \"\"\"\n",
    "        y = np.empty((self.batch_size, *self.dim), dtype=int)\n",
    "\n",
    "        image_ids = [self.image_indexes[i] for i in indexes]\n",
    "        df = self.data[self.data[\"id\"].isin(image_ids)]\n",
    "\n",
    "        decoded = df[\"annotation\"].apply(rle_decode)\n",
    "        df[\"x_cent\"], df[\"y_cent\"] = zip(*decoded.apply(get_centroid))\n",
    "        df[\"centroid\"] = list(zip(df[\"x_cent\"], df[\"y_cent\"]))\n",
    "        df[\"masks\"] = df[\"centroid\"].apply(make_mask)\n",
    "        y = df.groupby(\"id\")[\"masks\"].sum()\n",
    "\n",
    "        return np.stack(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 13:05:04.192526: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jamesholcombe/git/personal/cell-instance-segmentation/venv/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2021-12-27 13:05:04.192564: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-27 13:05:04.192580: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LDNLTCND1333CZ3): /proc/driver/nvidia/version does not exist\n",
      "2021-12-27 13:05:04.192789: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "x_in = Input(shape=(520, 704, 3))  # input shape: (height, width, 3 bands of RGB)\n",
    "\n",
    "\n",
    "x_temp = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x_in)\n",
    "x_temp = Dropout(0.25)(x_temp)\n",
    "x_temp = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x_temp)\n",
    "x_temp = MaxPooling2D((2, 2))(x_temp)\n",
    "x_temp = Conv2D(64, (3, 3), dilation_rate=(2, 2), activation=\"relu\", padding=\"same\")(\n",
    "    x_temp\n",
    ")  # dilated convolutions\n",
    "x_temp = Dropout(0.25)(x_temp)\n",
    "x_temp = Conv2D(64, (3, 3), dilation_rate=(2, 2), activation=\"relu\", padding=\"same\")(\n",
    "    x_temp\n",
    ")  # dilated convolutions\n",
    "x_temp = MaxPooling2D((2, 2))(x_temp)\n",
    "x_temp = Conv2D(64, (3, 3), dilation_rate=(2, 2), activation=\"relu\", padding=\"same\")(\n",
    "    x_temp\n",
    ")  # dilated convolutions\n",
    "x_temp = Dropout(0.25)(x_temp)\n",
    "x_temp = Conv2D(64, (3, 3), dilation_rate=(2, 2), activation=\"relu\", padding=\"same\")(\n",
    "    x_temp\n",
    ")  # dilated convolutions\n",
    "x_temp = MaxPooling2D((2, 2))(x_temp)\n",
    "x_temp = Conv2D(64, (3, 3), dilation_rate=(2, 2), activation=\"relu\", padding=\"same\")(\n",
    "    x_temp\n",
    ")  # dilated convolutions\n",
    "x_temp = Dropout(0.25)(x_temp)\n",
    "x_temp = Conv2D(64, (3, 3), dilation_rate=(2, 2), activation=\"relu\", padding=\"same\")(\n",
    "    x_temp\n",
    ")  # dilated convolutions\n",
    "\n",
    "x_temp = Conv2D(64, (1, 1), activation=\"relu\", padding=\"same\")(x_temp)\n",
    "x_temp = Conv2D(64, (1, 1), activation=\"relu\", padding=\"same\")(x_temp)\n",
    "x_out = Conv2D(1, (1, 1), activation=\"relu\", padding=\"same\")(x_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 520, 704, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 520, 704, 64)      1792      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 520, 704, 64)      0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 520, 704, 64)      36928     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 260, 352, 64)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 260, 352, 64)      36928     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 260, 352, 64)      0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 260, 352, 64)      36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 130, 176, 64)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 130, 176, 64)      36928     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 130, 176, 64)      0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 130, 176, 64)      36928     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 65, 88, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 65, 88, 64)        36928     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 65, 88, 64)        0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 65, 88, 64)        36928     \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 65, 88, 64)        4160      \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 65, 88, 64)        4160      \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 65, 88, 1)         65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 268,673\n",
      "Trainable params: 268,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def customLoss(yTrue, yPred):\n",
    "    return K.sqrt(\n",
    "        K.sum(\n",
    "            K.flatten(K.tf.multiply(K.square(yTrue - yPred), yTrue + 1) / (1 * 2 + 1))\n",
    "        )\n",
    "        / (64 * 64)\n",
    "    )\n",
    "\n",
    "\n",
    "model = Model(inputs=x_in, outputs=x_out)\n",
    "model.compile(loss=customLoss, optimizer=adam.Adam())  # setting loss and optimizer\n",
    "model.summary()  # printing the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_726/2111589972.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"x_cent\"], df[\"y_cent\"] = zip(*decoded.apply(get_centroid))\n",
      "/tmp/ipykernel_726/2111589972.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"centroid\"] = list(zip(df[\"x_cent\"], df[\"y_cent\"]))\n",
      "/tmp/ipykernel_726/2111589972.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"masks\"] = df[\"centroid\"].apply(make_mask)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 2/18 [==>...........................] - ETA: 9:35 - loss: 5.6084  "
     ]
    }
   ],
   "source": [
    "data_gen = DataGenerator(\n",
    "    DATAPATH / Path(\"train\"),\n",
    "    DATAPATH / Path(\"train/.csv\"),\n",
    ")\n",
    "model.fit(data_gen, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-24 17:13:53.949075: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Centroid model/assets\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"./models/Centroid_Estimation.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(\"./models/Centroid_Estimation.h5\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d38d43298adb51ba826b6e7da8ec99b7c56c60df20ebcb18ac2c959eba92e68"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
